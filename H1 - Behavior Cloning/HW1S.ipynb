{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLKNSALNOvMA"
   },
   "source": [
    "## Preparations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r-nJlOB4OvMB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from itertools import product               # Cartesian product for iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXqapovTOvMC"
   },
   "source": [
    "# Make sure you put all of the given python files in the same directories before running this cell!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "toi_mgdCOvMD"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import gridworld as W                       # basic grid-world MDPs\n",
    "import trajectory as T                      # trajectory generation\n",
    "import optimizer as O                       # stochastic gradient descent optimizer\n",
    "import solver as S                          # MDP solver (value-iteration)\n",
    "import plot as P                            # helper-functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8A4Wr3cEO6iX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipympl in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (0.9.3)\n",
      "Requirement already satisfied: ipython-genutils in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (7.6.5)\n",
      "Requirement already satisfied: matplotlib<4,>=3.4.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (3.4.3)\n",
      "Requirement already satisfied: numpy in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (1.24.3)\n",
      "Requirement already satisfied: pillow in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (8.4.0)\n",
      "Requirement already satisfied: traitlets<6 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (5.1.0)\n",
      "Requirement already satisfied: ipython<9 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipympl) (7.29.0)\n",
      "Requirement already satisfied: backcall in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (0.18.0)\n",
      "Requirement already satisfied: decorator in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (5.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (58.0.4)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (3.0.20)\n",
      "Requirement already satisfied: appnope in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (0.1.2)\n",
      "Requirement already satisfied: pygments in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipython<9->ipympl) (0.1.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.5.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (1.0.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (6.4.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (5.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.4.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (4.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (22.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->ipympl) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.10.0)\n",
      "Requirement already satisfied: six in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets<9,>=7.6.0->ipympl) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<9->ipympl) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (6.4.5)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (6.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (2.11.3)\n",
      "Requirement already satisfied: prometheus-client in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.3)\n",
      "Requirement already satisfied: testpath in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.3)\n",
      "Requirement already satisfied: bleach in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (4.0.0)\n",
      "Requirement already satisfied: defusedxml in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.4.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest-asyncio in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (1.10)\n",
      "Requirement already satisfied: webencodings in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/jamin/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.6.0->ipympl) (21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ffybDv77OvME"
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.rcParams['figure.figsize'] = [9, 5]     # set default figure size\n",
    "style = {                                   # global style for plots\n",
    "    'border': {'color': 'red', 'linewidth': 0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KE9M9MdddKdS"
   },
   "outputs": [],
   "source": [
    "def to_one_hot_array(input_array, num_unique_values):\n",
    "\n",
    "  num_unique_values = num_unique_values\n",
    "  one_hot_array = np.zeros((len(input_array), num_unique_values), dtype=int)\n",
    "\n",
    "  for i, val in enumerate(input_array):\n",
    "    one_hot_array[i, val] = 1\n",
    "\n",
    "  return one_hot_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMSOw6aruad5"
   },
   "source": [
    "# Setting up the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mRyK0vRcOvMF"
   },
   "outputs": [],
   "source": [
    "def setup_mdp(size, p_slip):\n",
    "    # create our world\n",
    "    world = W.IcyGridWorld(size= size, p_slip= p_slip)\n",
    "\n",
    "    # set up the reward function\n",
    "    reward = np.zeros(world.n_states) - 0.25\n",
    "    reward[-1] = 1.0\n",
    "    reward[8] = .6\n",
    "    reward[9] = -0.6\n",
    "\n",
    "    reward[5:8] = -0.6\n",
    "    reward[16:20] = -0.6\n",
    "\n",
    "    # set up terminal states\n",
    "    terminal = [24]\n",
    "\n",
    "    return world, reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Comment out the appropriate line of code\n",
    "\n",
    "### Deterministic Environment (Problem 1 & 3) ###\n",
    "# world, reward, terminal = setup_mdp(size = 5, p_slip = 0)\n",
    "\n",
    "# ### Stochastic Environment (Problem 2 & 4) ###\n",
    "world, reward, terminal = setup_mdp(size = 5, p_slip = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SAWjAjjOvMF"
   },
   "source": [
    "# Generate Expert Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wEx7nqklux_g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2 2 2 0 0 2 2 2 0 0 2 2 2 2 2 2 2 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def generate_expert_trajectories_optimal(world, reward, terminal, num_trajectories = 200):\n",
    "    n_trajectories = num_trajectories         # the number of \"expert\" trajectories\n",
    "    discount = 0.9              # discount for constructing an \"expert\" policy\n",
    "    weighting = lambda x: x**1  # down-weight less optimal actions\n",
    "    start = [0]                  # starting states for the expert\n",
    "\n",
    "    # compute the value-function\n",
    "    value, Q = S.value_iteration(world.p_transition, reward, discount)\n",
    "\n",
    "\n",
    "    # create our deterministic policy using the value function\n",
    "    policy = S.optimal_policy_from_value(world, value)\n",
    "    print(policy)\n",
    "\n",
    "\n",
    "    # a function that executes our deterministic policy by choosing actions according to it\n",
    "    policy_exec = T.policy_adapter(policy)\n",
    "\n",
    "\n",
    "    # generate trajectories\n",
    "    tjs = list(T.generate_trajectories(n_trajectories, world, policy_exec, start, terminal))\n",
    "\n",
    "    return tjs, policy, Q, True\n",
    "\n",
    "\n",
    "\n",
    "def generate_expert_trajectories_suboptimal(world, reward, terminal, num_trajectories = 200):\n",
    "    n_trajectories = num_trajectories         # the number of \"expert\" trajectories\n",
    "    discount = 0.9              # discount for constructing an \"expert\" policy\n",
    "    weighting = lambda x: x**1  # down-weight less optimal actions\n",
    "    start = [0]                  # starting states for the expert\n",
    "\n",
    "    # compute the value-function\n",
    "    value, Q = S.value_iteration(world.p_transition, reward, discount)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # create our stochastic policy using the value function\n",
    "    policy = S.stochastic_policy_from_value(world, value, w=weighting)\n",
    "    print(np.round(policy, 3))\n",
    "\n",
    "\n",
    "\n",
    "    # # a function that executes our stochastic policy by choosing actions according to it\n",
    "    policy_exec = T.stochastic_policy_adapter(policy)\n",
    "\n",
    "    # generate trajectories\n",
    "    tjs = list(T.generate_trajectories(n_trajectories, world, policy_exec, start, terminal))\n",
    "\n",
    "    return tjs, policy, Q, False\n",
    "\n",
    "\n",
    "#Comment out the appropriate Line of Code\n",
    "\n",
    "\n",
    "# Fully Optimal Expert (Problem 1)\n",
    "# trajectories, expert_policy, Q, optimal = generate_expert_trajectories_optimal(world, reward, terminal, num_trajectories = 20)\n",
    "\n",
    "# Fully Optimal Expert (Problem 2)\n",
    "trajectories, expert_policy, Q, optimal = generate_expert_trajectories_optimal(world, reward, terminal, num_trajectories = 40)\n",
    "\n",
    "\n",
    "# Suboptimal Expert (Problem 3)\n",
    "# trajectories, expert_policy, Q, optimal = generate_expert_trajectories_suboptimal(world, reward, terminal, num_trajectories = 10)\n",
    "\n",
    "\n",
    "# Suboptimal Expert (Problem 4)\n",
    "# trajectories, expert_policy, Q, optimal = generate_expert_trajectories_suboptimal(world, reward, terminal, num_trajectories = 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmeCHDpLCuHe"
   },
   "source": [
    "# Training a Behavioral Cloning Model on the trajectories of the expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X3LO15RDXSfR"
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aBZKKEWeXE5h"
   },
   "outputs": [],
   "source": [
    "for traject in trajectories:\n",
    "  for t in traject._t:\n",
    "    states.append(t[0])\n",
    "    actions.append(t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "442\n",
      "[0, 1, 2, 3, 8, 13, 18, 23, 0, 1, 2, 1, 2, 2, 2, 3, 8, 13, 18, 23, 0, 1, 0, 1, 2, 3, 8, 9, 14, 19, 0, 1, 2, 3, 8, 13, 18, 17, 18, 13, 18, 13, 18, 23, 0, 5, 10, 15, 20, 21, 21, 22, 17, 22, 23, 0, 1, 2, 3, 8, 13, 18, 23, 0, 1, 2, 3, 2, 2, 3, 8, 13, 18, 23, 0, 0, 1, 2, 3, 8, 13, 18, 13, 18, 23, 0, 1, 6, 7, 8, 13, 8, 13, 18, 23, 0, 0, 1, 2, 2, 2, 3, 8, 13, 12, 13, 14, 19, 0, 1, 0, 1, 2, 3, 4, 4, 9, 9, 8, 13, 18, 13, 18, 23, 0, 1, 0, 1, 2, 3, 8, 3, 8, 13, 18, 23, 22, 23, 0, 1, 2, 3, 8, 3, 8, 13, 8, 13, 12, 13, 18, 23, 0, 1, 2, 3, 8, 13, 18, 23, 0, 1, 2, 3, 2, 3, 8, 13, 18, 23, 23, 0, 1, 2, 1, 2, 3, 8, 13, 18, 17, 22, 23, 0, 1, 2, 3, 2, 3, 4, 9, 9, 4, 9, 14, 13, 18, 19, 0, 1, 2, 3, 2, 3, 8, 13, 18, 13, 18, 23, 18, 23, 0, 1, 2, 3, 4, 9, 14, 19, 0, 0, 1, 2, 2, 3, 3, 8, 13, 18, 23, 23, 0, 1, 2, 3, 8, 3, 8, 13, 18, 23, 23, 0, 0, 1, 2, 1, 2, 1, 2, 3, 8, 13, 8, 13, 12, 13, 18, 23, 0, 1, 2, 3, 8, 9, 14, 19, 0, 1, 6, 7, 8, 9, 14, 14, 19, 0, 5, 10, 15, 16, 21, 22, 23, 0, 1, 2, 3, 4, 9, 8, 13, 18, 23, 0, 1, 2, 3, 8, 13, 14, 19, 0, 1, 2, 3, 8, 13, 18, 23, 23, 0, 1, 2, 2, 3, 8, 13, 14, 19, 0, 1, 2, 1, 2, 3, 8, 13, 18, 23, 0, 1, 2, 3, 4, 9, 14, 19, 0, 1, 6, 7, 12, 13, 12, 13, 8, 7, 8, 7, 8, 13, 18, 13, 14, 19, 14, 19, 0, 1, 2, 3, 8, 13, 18, 17, 22, 23, 0, 1, 2, 3, 8, 13, 14, 9, 14, 19, 0, 0, 1, 1, 2, 1, 6, 7, 8, 9, 14, 9, 14, 19, 0, 1, 2, 3, 8, 13, 12, 13, 18, 23, 0, 1, 2, 7, 6, 7, 8, 9, 14, 19, 14, 19, 0, 5, 10, 11, 12, 13, 18, 17, 22, 23, 0, 1, 2, 7, 8, 13, 18, 19, 0, 1, 2, 3, 3, 8, 13, 18, 23]\n",
      "[0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(states))\n",
    "print(len(actions))\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuCxLkY1v2iP"
   },
   "source": [
    "# USE your favorite Supervised Learning Algorithm to train a model on the states and actions dataset where the states list is the input data and the actions list is the output data. Once you trained your model extract a policy from it for each of the 25 states and call it 'bc_policy'\n",
    "\n",
    "# Make sure you use 1-hot encoding for the states and actions spaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamin/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "inputs = list(range(25))\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_one_hot_array(input_array):\n",
    "    return (np.argmax(input_array, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vtXjKJjFwBQf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/14/zdcxt99s1kv_hw67vpgm8f640000gn/T/ipykernel_73107/3321030671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mbc_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#This should be an array of shape (25, ) with entries being either 0, 1, 2 or 3 (corresponding to the actions for each of the 25 states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mbc_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/14/zdcxt99s1kv_hw67vpgm8f640000gn/T/ipykernel_73107/3321030671.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(states, actions)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mencoded_state_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_one_hot_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mencoded_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_state_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdecoded_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_one_hot_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "### FILL IN THE CODE ########\n",
    "# Make sure you use 1-hot encoding for the states and actions spaces!\n",
    "\n",
    "def model (states, actions):\n",
    "    states_array = to_one_hot_array(states, 25)\n",
    "    actions_array = to_one_hot_array(actions, 4)\n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=(25, 4), max_iter=1000)\n",
    "    model.fit(states_array, actions_array)\n",
    "    \n",
    "    state_space = list(range(25))\n",
    "    encoded_state_space = to_one_hot_array(state_space, 25)\n",
    "    \n",
    "    encoded_policy = model_1.predict(encoded_state_space)\n",
    "    decoded_policy = from_one_hot_array(encoded_policy)\n",
    "    \n",
    "    return decoded_policy\n",
    "    \n",
    "\n",
    "bc_policy = model(states, actions)  #This should be an array of shape (25, ) with entries being either 0, 1, 2 or 3 (corresponding to the actions for each of the 25 states)\n",
    "\n",
    "assert bc_policy.shape == (25, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48nN0ow3zvhx"
   },
   "outputs": [],
   "source": [
    "bc_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRmXqpQzOvMG"
   },
   "source": [
    "Let's have a look at our world via the reward (left), expert policy (right) and trajectories (right, transparent white lines).\n",
    "The initial state for the trajectories is the bottom-left state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yh8Vs3pPuNO"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VulS1BCs2D_6"
   },
   "source": [
    "# Plot of the MDP (with corresponding reward and expert behavior)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izXw2z7rOvMH"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.title.set_text('Original Reward')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "p = P.plot_state_values(ax, world, reward, **style)\n",
    "fig.colorbar(p, cax=cax)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.title.set_text('Expert Policy and Trajectories')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "\n",
    "\n",
    "\n",
    "if optimal:\n",
    "  p = P.plot_stochastic_policy(ax, world, to_one_hot_array(expert_policy, 4), **style)\n",
    "else:\n",
    "  p = P.plot_stochastic_policy(ax, world, expert_policy, **style)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in trajectories:\n",
    "    P.plot_trajectory(ax, world, t, lw=5, color='white', alpha=0.025)\n",
    "\n",
    "\n",
    "fig.colorbar(p, cax=cax)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Khv_tZ0pOvML"
   },
   "source": [
    "# Plot the BC Policy (The red arrows on the left plot are the true optimal policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BWzSYRbITFj"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.title.set_text('Original Reward')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "p = P.plot_state_values(ax, world, reward, **style)\n",
    "P.plot_deterministic_policy(ax, world, S.optimal_policy(world, reward, 0.9), color='red')\n",
    "fig.colorbar(p, cax=cax)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.title.set_text('BC Policy')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "p = P.plot_stochastic_policy(ax, world, to_one_hot_array(bc_policy, 4), **style)\n",
    "\n",
    "fig.colorbar(p, cax=cax)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkncK3U_yuir"
   },
   "outputs": [],
   "source": [
    "def calculate_value_function(reward, dynamics, policy, discount_factor=0.9, tol=1e-6, max_iterations=1000):\n",
    "    num_states, _, num_actions = dynamics.shape\n",
    "    value_function = np.zeros(num_states)\n",
    "\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        prev_value_function = np.copy(value_function)\n",
    "\n",
    "        for state in range(num_states):\n",
    "            action_probs = policy[state, :]\n",
    "            action_value = 0\n",
    "\n",
    "            for action in range(num_actions):\n",
    "                next_state_values = np.sum(dynamics[state, :, action] * prev_value_function)\n",
    "                action_value += action_probs[action] * (reward[state] + discount_factor * next_state_values)\n",
    "\n",
    "            value_function[state] = action_value\n",
    "\n",
    "        if np.max(np.abs(value_function - prev_value_function)) < tol:\n",
    "            break\n",
    "\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2sHXByR4OUW"
   },
   "source": [
    "Value function with the true reward function to get an idea of the quality of the learnt policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnLtqNb9cdiJ"
   },
   "source": [
    "# Value function plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiwN4oNe3YMO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample 1D arrays of size 25\n",
    "if optimal:\n",
    "  values1 = calculate_value_function(reward, world.p_transition, to_one_hot_array(expert_policy, 4))\n",
    "else:\n",
    "  values1 = calculate_value_function(reward, world.p_transition, expert_policy)\n",
    "\n",
    "values3 = calculate_value_function(reward, world.p_transition, to_one_hot_array(bc_policy, 4))\n",
    "\n",
    "# Create subplots in a row\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Iterate through each subplot\n",
    "for ax, values in zip(axes, [values1, values3]):\n",
    "    # Rearrange the values to match the desired corner arrangement\n",
    "    values_arranged = np.flipud(values.reshape(5, 5))\n",
    "\n",
    "    # Use the 'viridis' colormap to map values to colors\n",
    "    im = ax.imshow(values_arranged, cmap='viridis', interpolation='nearest')\n",
    "\n",
    "    # Add text annotations for each cell\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax.text(j, i, f'{values_arranged[i, j]:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "    # Set title for each subplot\n",
    "    # ax.set_title(f'Array {values[0]:.2f} - {values[-1]:.2f}')\n",
    "\n",
    "    # Remove x and y ticks for better layout\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Add a colorbar\n",
    "cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Position of colorbar\n",
    "fig.colorbar(im, cax=cax)\n",
    "\n",
    "\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "# plt.tight_layout()\n",
    "\n",
    "axes[0].set_title('Expert Policy Value Function')\n",
    "axes[1].set_title('BC Policy Value Function')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHkha2yH1-pL"
   },
   "source": [
    "# References\n",
    "\n",
    "* [Abbel & Ng 2004]: https://doi.org/10.1145/1015330.1015430\n",
    "* [Bishop 2006]: https://www.springer.com/us/book/9780387310732\n",
    "* [Jaynes 1957]: https://doi.org/10.1103/physrev.106.620\n",
    "* [Kivinen et. al. 1997]: https://doi.org/10.1006/inco.1996.2612\n",
    "* [Ng et al. 99]: https://dl.acm.org/citation.cfm?id=645528.657613\n",
    "* [Osa et al. 2018]: https://arxiv.org/abs/1811.06711\n",
    "* [Ziebart et al. 2008]: http://www.cs.cmu.edu/~bziebart/publications/maximum-entropy-inverse-reinforcement-learning.html\n",
    "* [Ziebart 2010]: https://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf\n",
    "* Reused code from: https://github.com/qzed/irl-maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAsvnQ-HKNWG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
